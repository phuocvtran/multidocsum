{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing /mnt/HQ/uni/text-mining/multidocsum\n",
      "Requirement already satisfied: nltk==3.5 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from multidocsum==0.0.1) (3.5)\n",
      "Requirement already satisfied: numpy==1.19.4 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from multidocsum==0.0.1) (1.19.4)\n",
      "Requirement already satisfied: regex==2020.11.13 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from multidocsum==0.0.1) (2020.11.13)\n",
      "Requirement already satisfied: scikit-learn==0.21.3 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from multidocsum==0.0.1) (0.21.3)\n",
      "Requirement already satisfied: scipy==1.5.4 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from multidocsum==0.0.1) (1.5.4)\n",
      "Requirement already satisfied: underthesea==1.2.2 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from multidocsum==0.0.1) (1.2.2)\n",
      "Requirement already satisfied: pyvi==0.1 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from multidocsum==0.0.1) (0.1)\n",
      "Requirement already satisfied: pandas==1.1.4 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from multidocsum==0.0.1) (1.1.4)\n",
      "Requirement already satisfied: tqdm in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from nltk==3.5->multidocsum==0.0.1) (4.51.0)\n",
      "Requirement already satisfied: click in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from nltk==3.5->multidocsum==0.0.1) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from nltk==3.5->multidocsum==0.0.1) (0.17.0)\n",
      "Requirement already satisfied: requests in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from underthesea==1.2.2->multidocsum==0.0.1) (2.25.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.6 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from underthesea==1.2.2->multidocsum==0.0.1) (0.9.7)\n",
      "Requirement already satisfied: tabulate in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from underthesea==1.2.2->multidocsum==0.0.1) (0.8.7)\n",
      "Requirement already satisfied: unidecode in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from underthesea==1.2.2->multidocsum==0.0.1) (1.1.1)\n",
      "Requirement already satisfied: sklearn-crfsuite in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from pyvi==0.1->multidocsum==0.0.1) (0.3.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from pandas==1.1.4->multidocsum==0.0.1) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from pandas==1.1.4->multidocsum==0.0.1) (2020.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from requests->underthesea==1.2.2->multidocsum==0.0.1) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from requests->underthesea==1.2.2->multidocsum==0.0.1) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from requests->underthesea==1.2.2->multidocsum==0.0.1) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from requests->underthesea==1.2.2->multidocsum==0.0.1) (3.0.4)\n",
      "Requirement already satisfied: six in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from sklearn-crfsuite->pyvi==0.1->multidocsum==0.0.1) (1.15.0)\n",
      "Building wheels for collected packages: multidocsum\n",
      "  Building wheel for multidocsum (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for multidocsum: filename=multidocsum-0.0.1-py3-none-any.whl size=8467 sha256=ac80a858acda8fc054afc945a174915217a3a18ed74f3638002765ede6d827c4\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-o0mr3yf4/wheels/02/74/95/c7b282f469bd75d8476184f75ec79b255b390a90201a66b918\n",
      "Successfully built multidocsum\n",
      "Installing collected packages: multidocsum\n",
      "  Attempting uninstall: multidocsum\n",
      "    Found existing installation: multidocsum 0.0.1\n",
      "    Uninstalling multidocsum-0.0.1:\n",
      "      Successfully uninstalled multidocsum-0.0.1\n",
      "Successfully installed multidocsum-0.0.1\n",
      "Requirement already satisfied: rouge-score in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (0.0.4)\n",
      "Requirement already satisfied: absl-py in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from rouge-score) (0.11.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from rouge-score) (1.15.0)\n",
      "Requirement already satisfied: nltk in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from rouge-score) (3.5)\n",
      "Requirement already satisfied: numpy in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from rouge-score) (1.19.4)\n",
      "Requirement already satisfied: click in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from nltk->rouge-score) (7.1.2)\n",
      "Requirement already satisfied: regex in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from nltk->rouge-score) (2020.11.13)\n",
      "Requirement already satisfied: tqdm in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from nltk->rouge-score) (4.51.0)\n",
      "Requirement already satisfied: joblib in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from nltk->rouge-score) (0.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ..\n",
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.extracting import MEAD\n",
    "from src.data.data_io import DataIO\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_name = []\n",
    "for path in glob('../data/interim/extract_content/*'):\n",
    "    cluster_name.append(path.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 26min 23s, sys: 795 ms, total: 26min 24s\nWall time: 26min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "my_sum = {}\n",
    "gold_sum = {}\n",
    "for name in cluster_name:\n",
    "    params = {\n",
    "        'data_path': f'../data/interim/extract_content/{name}',\n",
    "        'stop_words_path': '../data/external/stopwords.txt', \n",
    "        'punc_path': '../data/external/punctuation.txt',\n",
    "        'compression_rate': 0.05,\n",
    "        'remove_redundancy': True,\n",
    "        'wc': 1.0, \n",
    "        'wp': 1.0, \n",
    "        'wf': 1.0, \n",
    "        'n_centroid': 10, \n",
    "        'C_max': 10, \n",
    "        'ngram_range': (1, 1), \n",
    "        'max_features': 100,\n",
    "        'func': np.dot\n",
    "    }\n",
    "    summary = MEAD.extract(**params)\n",
    "    os.makedirs(f'../summaries/{name}', exist_ok=True)\n",
    "    with open(f'../summaries/{name}/summary.txt', 'w') as f:\n",
    "        f.write('\\n'.join(summary))\n",
    "        my_sum[name] = '\\n'.join(summary)\n",
    "        gold_sum[name] = DataIO.load(f'../data/raw/ViMs/summary/{name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rougen = ['rouge1', 'rouge2', 'rouge3', 'rouge4']\n",
    "scorer = rouge_scorer.RougeScorer(rougen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 12.5 s, sys: 0 ns, total: 12.5 s\nWall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mean_0 = {}\n",
    "for n in rougen:\n",
    "    mean_0[n] = {}\n",
    "    rn = mean_0[n]\n",
    "    precision = []\n",
    "    recall = []\n",
    "    fmeasure = []\n",
    "    for cluster in my_sum:\n",
    "        precision.append(scorer.score(gold_sum[cluster]['0.gold.txt'], my_sum[cluster])[n].precision)\n",
    "        recall.append(scorer.score(gold_sum[cluster]['0.gold.txt'], my_sum[cluster])[n].recall)\n",
    "        fmeasure.append(scorer.score(gold_sum[cluster]['0.gold.txt'], my_sum[cluster])[n].fmeasure)\n",
    "    rn['precision'] = np.mean(precision)\n",
    "    rn['recall'] = np.mean(recall)\n",
    "    rn['f-measure'] = np.mean(fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             rouge1    rouge2    rouge3    rouge4\n",
       "precision  0.709228  0.445738  0.304384  0.252520\n",
       "recall     0.724331  0.457173  0.311050  0.257734\n",
       "f-measure  0.670701  0.421880  0.288323  0.239300"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rouge3</th>\n      <th>rouge4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>precision</th>\n      <td>0.709228</td>\n      <td>0.445738</td>\n      <td>0.304384</td>\n      <td>0.252520</td>\n    </tr>\n    <tr>\n      <th>recall</th>\n      <td>0.724331</td>\n      <td>0.457173</td>\n      <td>0.311050</td>\n      <td>0.257734</td>\n    </tr>\n    <tr>\n      <th>f-measure</th>\n      <td>0.670701</td>\n      <td>0.421880</td>\n      <td>0.288323</td>\n      <td>0.239300</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(mean_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_1 = {}\n",
    "for n in rougen:\n",
    "    mean_1[n] = {}\n",
    "    rn = mean_1[n]\n",
    "    precision = []\n",
    "    recall = []\n",
    "    fmeasure = []\n",
    "    for cluster in my_sum:\n",
    "        precision.append(scorer.score(gold_sum[cluster]['1.gold.txt'], my_sum[cluster])[n].precision)\n",
    "        recall.append(scorer.score(gold_sum[cluster]['1.gold.txt'], my_sum[cluster])[n].recall)\n",
    "        fmeasure.append(scorer.score(gold_sum[cluster]['1.gold.txt'], my_sum[cluster])[n].fmeasure)\n",
    "    rn['precision'] = np.mean(precision)\n",
    "    rn['recall'] = np.mean(recall)\n",
    "    rn['f-measure'] = np.mean(fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             rouge1    rouge2    rouge3    rouge4\n",
       "precision  0.762609  0.490683  0.346229  0.291536\n",
       "recall     0.704133  0.453261  0.318807  0.268423\n",
       "f-measure  0.696245  0.447542  0.315653  0.265948"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rouge3</th>\n      <th>rouge4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>precision</th>\n      <td>0.762609</td>\n      <td>0.490683</td>\n      <td>0.346229</td>\n      <td>0.291536</td>\n    </tr>\n    <tr>\n      <th>recall</th>\n      <td>0.704133</td>\n      <td>0.453261</td>\n      <td>0.318807</td>\n      <td>0.268423</td>\n    </tr>\n    <tr>\n      <th>f-measure</th>\n      <td>0.696245</td>\n      <td>0.447542</td>\n      <td>0.315653</td>\n      <td>0.265948</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "pd.DataFrame(mean_1)"
   ]
  }
 ]
}