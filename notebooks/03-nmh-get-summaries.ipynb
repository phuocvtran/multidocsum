{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing /mnt/HQ/uni/text-mining/multidocsum\n",
      "Requirement already satisfied: nltk==3.5 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from multidocsum==0.0.1) (3.5)\n",
      "Requirement already satisfied: numpy==1.19.4 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from multidocsum==0.0.1) (1.19.4)\n",
      "Requirement already satisfied: regex==2020.11.13 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from multidocsum==0.0.1) (2020.11.13)\n",
      "Requirement already satisfied: scikit-learn==0.21.3 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from multidocsum==0.0.1) (0.21.3)\n",
      "Requirement already satisfied: scipy==1.5.4 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from multidocsum==0.0.1) (1.5.4)\n",
      "Requirement already satisfied: underthesea==1.2.2 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from multidocsum==0.0.1) (1.2.2)\n",
      "Requirement already satisfied: pyvi==0.1 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from multidocsum==0.0.1) (0.1)\n",
      "Requirement already satisfied: pandas==1.1.4 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from multidocsum==0.0.1) (1.1.4)\n",
      "Requirement already satisfied: click in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from nltk==3.5->multidocsum==0.0.1) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from nltk==3.5->multidocsum==0.0.1) (0.17.0)\n",
      "Requirement already satisfied: tqdm in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from nltk==3.5->multidocsum==0.0.1) (4.51.0)\n",
      "Requirement already satisfied: requests in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from underthesea==1.2.2->multidocsum==0.0.1) (2.25.0)\n",
      "Requirement already satisfied: unidecode in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from underthesea==1.2.2->multidocsum==0.0.1) (1.1.1)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.6 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from underthesea==1.2.2->multidocsum==0.0.1) (0.9.7)\n",
      "Requirement already satisfied: tabulate in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from underthesea==1.2.2->multidocsum==0.0.1) (0.8.7)\n",
      "Requirement already satisfied: sklearn-crfsuite in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from pyvi==0.1->multidocsum==0.0.1) (0.3.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from pandas==1.1.4->multidocsum==0.0.1) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from pandas==1.1.4->multidocsum==0.0.1) (2020.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from requests->underthesea==1.2.2->multidocsum==0.0.1) (1.26.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from requests->underthesea==1.2.2->multidocsum==0.0.1) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from requests->underthesea==1.2.2->multidocsum==0.0.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from requests->underthesea==1.2.2->multidocsum==0.0.1) (2.10)\n",
      "Requirement already satisfied: six in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from sklearn-crfsuite->pyvi==0.1->multidocsum==0.0.1) (1.15.0)\n",
      "Building wheels for collected packages: multidocsum\n",
      "  Building wheel for multidocsum (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for multidocsum: filename=multidocsum-0.0.1-py3-none-any.whl size=8429 sha256=1988f7f318a101fb70a90d04060d3480a5c81e1f42eff99cc0ab123e59fd8739\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-aaq526b8/wheels/02/74/95/c7b282f469bd75d8476184f75ec79b255b390a90201a66b918\n",
      "Successfully built multidocsum\n",
      "Installing collected packages: multidocsum\n",
      "  Attempting uninstall: multidocsum\n",
      "    Found existing installation: multidocsum 0.0.1\n",
      "    Uninstalling multidocsum-0.0.1:\n",
      "      Successfully uninstalled multidocsum-0.0.1\n",
      "Successfully installed multidocsum-0.0.1\n",
      "Requirement already satisfied: rouge-score in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (0.0.4)\n",
      "Requirement already satisfied: nltk in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from rouge-score) (3.5)\n",
      "Requirement already satisfied: absl-py in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from rouge-score) (0.11.0)\n",
      "Requirement already satisfied: numpy in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from rouge-score) (1.19.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from rouge-score) (1.15.0)\n",
      "Requirement already satisfied: regex in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from nltk->rouge-score) (2020.11.13)\n",
      "Requirement already satisfied: tqdm in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from nltk->rouge-score) (4.51.0)\n",
      "Requirement already satisfied: click in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from nltk->rouge-score) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/miu/miniconda3/envs/text-mining/lib/python3.7/site-packages (from nltk->rouge-score) (0.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ..\n",
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.extracting import Extractor\n",
    "from src.data.data_io import DataIO\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_name = []\n",
    "for path in glob('../data/interim/extract_content/*'):\n",
    "    cluster_name.append(path.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 29min 37s, sys: 1.06 s, total: 29min 38s\nWall time: 30min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "my_sum = {}\n",
    "gold_sum = {}\n",
    "for name in cluster_name:\n",
    "    params = {\n",
    "        'data_path': f'../data/interim/extract_content/{name}',\n",
    "        'stop_words_path': '../data/external/stopwords.txt', \n",
    "        'punc_path': '../data/external/punctuation.txt',\n",
    "        'compression_rate': 0.05,\n",
    "        'remove_redundacy': True,\n",
    "        'wc': 1.0, \n",
    "        'wp': 1.0, \n",
    "        'wf': 1.0, \n",
    "        'n_centroid': 10, \n",
    "        'C_max': 10, \n",
    "        'ngram_range': (1, 1), \n",
    "        'max_features': 100,\n",
    "        'func': np.dot\n",
    "    }\n",
    "    summary = Extractor.extract(**params)\n",
    "    os.makedirs(f'../summaries/{name}', exist_ok=True)\n",
    "    with open(f'../summaries/{name}/summary.txt', 'w') as f:\n",
    "        f.write('\\n'.join(summary))\n",
    "        my_sum[name] = '\\n'.join(summary)\n",
    "        gold_sum[name] = DataIO.load(f'../data/raw/ViMs/summary/{name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rougen = ['rouge1', 'rouge2', 'rouge3', 'rouge4']\n",
    "scorer = rouge_scorer.RougeScorer(rougen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 13.8 s, sys: 6.81 ms, total: 13.8 s\nWall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mean_0 = {}\n",
    "for n in rougen:\n",
    "    mean_0[n] = {}\n",
    "    rn = mean_0[n]\n",
    "    precision = []\n",
    "    recall = []\n",
    "    fmeasure = []\n",
    "    for cluster in my_sum:\n",
    "        precision.append(scorer.score(gold_sum[cluster]['0.gold.txt'], my_sum[cluster])[n].precision)\n",
    "        recall.append(scorer.score(gold_sum[cluster]['0.gold.txt'], my_sum[cluster])[n].recall)\n",
    "        fmeasure.append(scorer.score(gold_sum[cluster]['0.gold.txt'], my_sum[cluster])[n].fmeasure)\n",
    "    rn['precision'] = np.mean(precision)\n",
    "    rn['recall'] = np.mean(recall)\n",
    "    rn['f-measure'] = np.mean(fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             rouge1    rouge2    rouge3    rouge4\n",
       "precision  0.629926  0.405517  0.276815  0.228640\n",
       "recall     0.804274  0.520454  0.347108  0.282270\n",
       "f-measure  0.664363  0.428040  0.290197  0.238402"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rouge3</th>\n      <th>rouge4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>precision</th>\n      <td>0.629926</td>\n      <td>0.405517</td>\n      <td>0.276815</td>\n      <td>0.228640</td>\n    </tr>\n    <tr>\n      <th>recall</th>\n      <td>0.804274</td>\n      <td>0.520454</td>\n      <td>0.347108</td>\n      <td>0.282270</td>\n    </tr>\n    <tr>\n      <th>f-measure</th>\n      <td>0.664363</td>\n      <td>0.428040</td>\n      <td>0.290197</td>\n      <td>0.238402</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(mean_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 15 s, sys: 13.2 ms, total: 15 s\nWall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mean_1 = {}\n",
    "for n in rougen:\n",
    "    mean_1[n] = {}\n",
    "    rn = mean_1[n]\n",
    "    precision = []\n",
    "    recall = []\n",
    "    fmeasure = []\n",
    "    for cluster in my_sum:\n",
    "        precision.append(scorer.score(gold_sum[cluster]['1.gold.txt'], my_sum[cluster])[n].precision)\n",
    "        recall.append(scorer.score(gold_sum[cluster]['1.gold.txt'], my_sum[cluster])[n].recall)\n",
    "        fmeasure.append(scorer.score(gold_sum[cluster]['1.gold.txt'], my_sum[cluster])[n].fmeasure)\n",
    "    rn['precision'] = np.mean(precision)\n",
    "    rn['recall'] = np.mean(recall)\n",
    "    rn['f-measure'] = np.mean(fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             rouge1    rouge2    rouge3    rouge4\n",
       "precision  0.677139  0.421916  0.277088  0.222519\n",
       "recall     0.783271  0.492738  0.320094  0.255440\n",
       "f-measure  0.690738  0.430924  0.281784  0.225637"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rouge3</th>\n      <th>rouge4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>precision</th>\n      <td>0.677139</td>\n      <td>0.421916</td>\n      <td>0.277088</td>\n      <td>0.222519</td>\n    </tr>\n    <tr>\n      <th>recall</th>\n      <td>0.783271</td>\n      <td>0.492738</td>\n      <td>0.320094</td>\n      <td>0.255440</td>\n    </tr>\n    <tr>\n      <th>f-measure</th>\n      <td>0.690738</td>\n      <td>0.430924</td>\n      <td>0.281784</td>\n      <td>0.225637</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "pd.DataFrame(mean_1)"
   ]
  }
 ]
}